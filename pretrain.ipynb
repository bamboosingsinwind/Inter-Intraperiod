{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mkzORFQhvWGQ"
      },
      "outputs": [],
      "source": [
        "#@title Load packages and data\n",
        "\n",
        "import torch as th\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from sktime.datasets import load_gunpoint\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import wandb\n",
        "wandb.init(\n",
        "    project=\"ssl-ourECG\",\n",
        "    config={\n",
        "    \"learning_rate\": 0.0003,\n",
        "    \"architecture\": \"resnet18to34+fcn\",  #  \n",
        "    \"dataset\": \"BTCH_rhythm_median_8lead\", #ii_v1\n",
        "    \"epochs\": 30, #30  100\n",
        "    }\n",
        ")\n",
        "def to_np(x):\n",
        "    return x.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5tNCbzc23edS"
      },
      "outputs": [],
      "source": [
        "from dataset_pretrain import pretrain_set as training_set\n",
        "from dataset_pretrain import valid_set,test_set\n",
        "print(training_set[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fcn import FCN\n",
        "from  onedResnet_myopt import resnet18to34\n",
        "import torch.nn as nn\n",
        "class MyModelWithoutFC(nn.Module):#\n",
        "    def __init__(self,out_ch):\n",
        "        super(MyModelWithoutFC, self).__init__()\n",
        "        self.out_ch = out_ch\n",
        "        self.encoder = resnet18to34(num_classes=2)\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(self.out_ch, self.out_ch),\n",
        "            nn.BatchNorm1d(self.out_ch),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.out_ch, 2)\n",
        "        )\n",
        "        self.proj_head = nn.Sequential(\n",
        "            nn.Linear(self.out_ch, self.out_ch),\n",
        "            nn.BatchNorm1d(self.out_ch),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.out_ch, self.out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): \n",
        "        h = self.encoder(x)  \n",
        "        out = self.proj_head(h)\n",
        "        rr = self.predictor(h)\n",
        "        return out, h, rr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from NT_Xent import NT_Xent_loss\n",
        "mse_loss = nn.MSELoss()\n",
        "th.cuda.is_available()\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "def eval_fn(target,pred):\n",
        "    cm = confusion_matrix(target, pred)\n",
        "    acc = (cm[0][0]+cm[1][1])/cm.sum()\n",
        "    print(\"acc\",acc)\n",
        "    print(\"cm\",cm)\n",
        "    sen,spe = cm[1][1]/cm[1].sum(),cm[0][0]/cm[0].sum()\n",
        "    wandb.log({\"Accuracy\":acc,\"Sensitivity\":sen,\"Specificity\":spe})\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Vppl1UTbx6xk"
      },
      "outputs": [],
      "source": [
        "#@title mixup model trainer per epoch\n",
        "from transformations import *\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "save_dir = \"./Inter-Intraperiod-ECG/save_models/\"  \n",
        "# best_acc = 0\n",
        "def train_mixup_model_epoch(model1,model2, training_set, test_set, optimizer1,optimizer2,  scheduler1,scheduler2,alpha, epochs):\n",
        "    \n",
        "    device = 'cuda' if th.cuda.is_available() else 'cpu'\n",
        "    batch_size_tr = len(training_set)#len(training_set)#len(training_set)#len(training_set)#len(training_set.x)\n",
        "    bs = 16\n",
        "    best_acc = 0\n",
        "    training_generator = DataLoader(training_set, batch_size=bs,\n",
        "                                    shuffle=True, drop_last=True)\n",
        "    for epoch in range(epochs):\n",
        "        for rhythm,median,feat_rr,label in tqdm(training_generator,desc=f'Epoch {epoch + 1}/{epochs}'):\n",
        "            \n",
        "            rhythm,median,feat_rr = rhythm.to(device),median.to(device),feat_rr.to(device)\n",
        "            model1.train()\n",
        "            model2.train()\n",
        "            optimizer1.zero_grad()\n",
        "            optimizer2.zero_grad()\n",
        "            \n",
        "            p1,h1,rr = model1(rhythm)\n",
        "            p2,h2 = model2(median)\n",
        "           \n",
        "            loss1 = NT_Xent_loss(p1,p2,temperature=0.5)\n",
        "            loss2 = mse_loss(rr,feat_rr)\n",
        "            loss = alpha*loss1 + (1-alpha)*loss2\n",
        "            loss.backward()\n",
        "            optimizer1.step()\n",
        "            optimizer2.step()\n",
        "            lr1 = optimizer1.param_groups[0]['lr']\n",
        "            lr2 = optimizer2.param_groups[0]['lr']\n",
        "            wandb.log({\"loss_pred\":loss2.item(),\"loss_contrast\":loss1.item(),\"loss_all\":loss.item(),\n",
        "                       \"lr1\":lr1,\"lr2\":lr2})\n",
        "        scheduler1.step()\n",
        "        scheduler2.step()\n",
        "        acc = test_model(model1, valid_set, test_set) \n",
        "        # wandb.log({\"Accuracy\":acc})\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            th.save(model1.state_dict(), save_dir+\"resnet18to34_best_acc.pth\") \n",
        "            th.save(model2.state_dict(), save_dir+\"fcn_best_acc.pth\")\n",
        "            print(\"update saving the model\")\n",
        "        if epoch == epochs - 1:\n",
        "            th.save(model1.state_dict(), save_dir+\"resnet18to34_final.pth\")\n",
        "            th.save(model2.state_dict(), save_dir+\"fcn_final.pth\")\n",
        "            print(\"end\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SSwqzgbEx8-B"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test_model(model, valid_set, test_set):\n",
        "    out_ch = 512 #128\n",
        "    batch_size = 200\n",
        "    model.eval()\n",
        "\n",
        "    N_tr = len(valid_set)#len(training_set.x)\n",
        "    N_te = len(test_set) #len(test_set.x)\n",
        "\n",
        "    valid_generator = DataLoader(valid_set, batch_size=batch_size,\n",
        "                                    shuffle=True, drop_last=False)\n",
        "    test_generator = DataLoader(test_set, batch_size= batch_size,\n",
        "                                    shuffle=True, drop_last=False)\n",
        "\n",
        "    H_tr = th.zeros((N_tr, out_ch))\n",
        "    y_tr = th.zeros((N_tr), dtype=th.long)\n",
        "\n",
        "    H_te = th.zeros((N_te, out_ch))\n",
        "    y_te = th.zeros((N_te), dtype=th.long)\n",
        "\n",
        "    for batch_idx, (x_tr, _, _, y_tr_i) in enumerate(valid_generator):\n",
        "        with torch.no_grad():\n",
        "            x_tr = x_tr.to(\"cuda\")\n",
        "            _, H_tr_i, _ = model(x_tr)\n",
        "            H_tr[batch_idx * batch_size: (batch_idx + 1) * batch_size] = H_tr_i\n",
        "            y_tr[batch_idx * batch_size: (batch_idx + 1) * batch_size] = y_tr_i\n",
        "\n",
        "    H_tr = to_np(nn.functional.normalize(H_tr))\n",
        "    y_tr = to_np(y_tr)\n",
        "\n",
        "    for batch_idx, (x_te, _, _, y_te_i) in enumerate(test_generator):\n",
        "        with torch.no_grad():\n",
        "            x_te = x_te.to(\"cuda\")\n",
        "            _, H_te_i, _ = model(x_te)\n",
        "            H_te[batch_idx * batch_size: (batch_idx + 1) * batch_size] = H_te_i\n",
        "            y_te[batch_idx * batch_size: (batch_idx + 1) * batch_size] = y_te_i\n",
        "    \n",
        "\n",
        "    H_te = to_np(nn.functional.normalize(H_te))\n",
        "    y_te = to_np(y_te)\n",
        "\n",
        "    clf = KNeighborsClassifier(n_neighbors=1).fit(H_tr, y_tr) #对于FCN扩大k1to3 能提点\n",
        "\n",
        "    pred = clf.predict(H_te)#weighted KNN\n",
        "    acc = eval_fn(y_te,pred)\n",
        "    return acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "227XYyoHx--y"
      },
      "outputs": [],
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "device = 'cuda' if th.cuda.is_available() else 'cpu'\n",
        "epochs = 30 #100\n",
        "alpha = 0.5 #0.5\n",
        "\n",
        "model1 = MyModelWithoutFC(out_ch=512).to(device) #512+2\n",
        "model2 = FCN(n_in=2).to(device)  #n_in=8\n",
        "optimizer1 = th.optim.Adam(model1.parameters(),lr=0.0003)\n",
        "optimizer2 = th.optim.Adam(model2.parameters(),lr=0.0003)\n",
        "scheduler1 = torch.optim.lr_scheduler.StepLR(optimizer1,step_size=3,gamma=0.9)\n",
        "scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer2,step_size=3,gamma=0.9)\n",
        "train_mixup_model_epoch(model1,model2, training_set, test_set,\n",
        "                                              optimizer1,optimizer2, scheduler1,scheduler2,alpha, epochs)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MixupContrastiveLearningExample.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
